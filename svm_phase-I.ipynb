{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import pandas as pd\n",
    "import PyPDF2 as pdf\n",
    "import re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intializing file variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_1 = 'Solverminds_Data/Sample/001.pdf'\n",
    "file_2 = 'Solverminds_Data/Sample/002.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing PDF files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyPDF2 is a pure-python PDF library capable of splitting, merging together, cropping, and transforming the pages of PDF files. It can also add custom data, viewing options, and passwords to PDF files. It can retrieve text and metadata from PDFs as well as merge entire files together. Here we used PyPDF2 to handel data from pdf files.\n",
    "Next we use python 're' module, stands for regular expression to clean the text obtained from pdf files. In the function below regular expressions are used to remove escape sequences, white spaces and punctuations. We use simple replace function of strin object to remove new lines from the text.\n",
    "Finally the function returns cleaned text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processPdf(file):\n",
    "    filereader = pdf.PdfFileReader(open(file, 'rb'))\n",
    "    count = 0\n",
    "    while count < filereader.getNumPages():\n",
    "        pageObj = filereader.getPage(count)\n",
    "        count += 1\n",
    "        text = pageObj.extractText()\n",
    "\n",
    "        #Punctuation removal\n",
    "        processed = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        processed = processed.replace('\\n', ' ')\n",
    "        processed = re.sub(' +', ' ', processed)\n",
    "        processed = processed.strip()\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating instances of spaCy and NLTK models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "doc = nlp(processPdf(file_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "The next step in our pipeline is to break this sentence into separate words or tokens. This is called tokenization. Tokenization is easy to do in English. We’ll just split apart words whenever there’s a space between them. And we’ll also treat punctuation marks as separate tokens since punctuation also has meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = [token.text for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords Removal\n",
    "Next, we want to consider the importance of each word in the sentence. English has a lot of filler words that appear very frequently like “and”, “the”, and “a”. When doing statistics on text, these words introduce a lot of noise since they appear way more frequently than other words. Some NLP pipelines will flag them as stop words —that is, words that we might want to filter out before doing any statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopwords Removal\n",
    "word_tokens_stop = [token.text for token in doc if not token.is_stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization\n",
    "For grammatical reasons, documents are going to use different forms of a word, such as organize, organizes, and organizing. Additionally, there are families of derivationally related words with similar meanings, such as democracy, democratic, and democratization. In many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set.\n",
    "\n",
    "The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "word_tokens_stem = [stemmer.stem(token.text) for token in doc if not token.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization\n",
    "word_tokens_lemma = [token.lemma_ for token in doc if not token.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Vectorization\n",
    "tf-idf stands for Term frequency-inverse document frequency. The tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus (data-set).\n",
    "\n",
    "tf-idf is a weighting scheme that assigns each term in a document a weight based on its term frequency (tf) and inverse document frequency (idf). The terms with higher weight scores are considered to be more important.\n",
    "\n",
    "Typically, the tf-idf weight is composed by two terms-\n",
    "\n",
    "Normalized Term Frequency (tf)\n",
    "Inverse Document Frequency (idf)\n",
    "\n",
    "###### Term Frequency (tf):\n",
    "Gives us the frequency of the word in each document in the corpus. It is the ratio of number of times the word appears in a document compared to the total number of words in that document. It increases as the number of occurrences of that word within the document increases. Each document has its own tf.\n",
    "$$tf_{i,j}=\\frac{n_{i,j}}{\\sum_k n_{i,j}}$$\n",
    "\n",
    "###### Inverse Data Frequency (idf):\n",
    "Used to calculate the weight of rare words across all documents in the corpus. The words that occur rarely in the corpus have a high IDF score. It is given by the equation below.\n",
    "$$idf(w) = log\\left (\\frac{N}{df_t}  \\right )$$\n",
    "\n",
    "\n",
    "Combining these two we come up with the TF-IDF score (w) for a word in a document in the corpus. It is the product of tf and idf:\n",
    "$$w_{i,j} = tf_{i,j} \\times log\\left (\\frac{N}{df_i}  \\right )$$\n",
    "\n",
    "$$tf_{i,j} = \\textrm{number of occurences of i in j}$$\n",
    "$$df_i = \\textrm{number of documents conatining i}$$\n",
    "$$N = \\textrm{Toat number of documents}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "docA = processPdf(file_1)\n",
    "docB = processPdf(file_2)\n",
    "docB = \"the dog sat on my bed\"\n",
    "\n",
    "bowA = docA.split(\" \")\n",
    "bowB = docB.split(\" \")\n",
    "\n",
    "wordSet = set(bowA).union(set(bowB))\n",
    "\n",
    "wordDictA = dict.fromkeys(wordSet, 0)\n",
    "wordDictB = dict.fromkeys(wordSet, 0)\n",
    "\n",
    "for word in bowA:\n",
    "    wordDictA[word] += 1\n",
    "    \n",
    "for word in bowB:\n",
    "    wordDictB[word] +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function 'computeTF' computes the TF score for each word in the corpus, by document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, bow):\n",
    "    tfDict = {}\n",
    "    bowCount = len(bow)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bowCount)\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfBowA = computeTF(wordDictA, bowA)\n",
    "tfBowB = computeTF(wordDictB, bowB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function 'computeIDF' computes the IDF score of every word in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(docList):\n",
    "    import math\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    \n",
    "    #counts the number of documents that contain a word w\n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for doc in docList:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "                \n",
    "    #Divide N by denominator above, take the log of that\n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "        \n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfs = computeIDF([wordDictA, wordDictB])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function 'computeTFIDF' below computes the TF-IDF score for each word, by multiplying the TF and IDF scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfBowA = computeTFIDF(tfBowA, idfs)\n",
    "tfidfBowB = computeTFIDF(tfBowB, idfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The output produced by the above code for the set of documents file_1 and file_2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>are</th>\n",
       "      <th>Americas</th>\n",
       "      <th>periodically</th>\n",
       "      <th>properly</th>\n",
       "      <th>Republic</th>\n",
       "      <th>ensure</th>\n",
       "      <th>sat</th>\n",
       "      <th>U</th>\n",
       "      <th>concerned</th>\n",
       "      <th>concerning</th>\n",
       "      <th>...</th>\n",
       "      <th>Consular</th>\n",
       "      <th>Avenue</th>\n",
       "      <th>June</th>\n",
       "      <th>format</th>\n",
       "      <th>4</th>\n",
       "      <th>Finance</th>\n",
       "      <th>will</th>\n",
       "      <th>considered</th>\n",
       "      <th>and</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003094</td>\n",
       "      <td>0.003094</td>\n",
       "      <td>0.003094</td>\n",
       "      <td>0.003094</td>\n",
       "      <td>0.003094</td>\n",
       "      <td>0.003094</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003094</td>\n",
       "      <td>0.003094</td>\n",
       "      <td>0.003094</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012378</td>\n",
       "      <td>0.003094</td>\n",
       "      <td>0.003094</td>\n",
       "      <td>0.003094</td>\n",
       "      <td>0.003094</td>\n",
       "      <td>0.003094</td>\n",
       "      <td>0.006189</td>\n",
       "      <td>0.003094</td>\n",
       "      <td>0.040227</td>\n",
       "      <td>0.009283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        are  Americas  periodically  properly  Republic    ensure       sat  \\\n",
       "0  0.003094  0.003094      0.003094  0.003094  0.003094  0.003094  0.000000   \n",
       "1  0.000000  0.000000      0.000000  0.000000  0.000000  0.000000  0.115525   \n",
       "\n",
       "          U  concerned  concerning  ...  Consular    Avenue      June  \\\n",
       "0  0.003094   0.003094    0.003094  ...  0.012378  0.003094  0.003094   \n",
       "1  0.000000   0.000000    0.000000  ...  0.000000  0.000000  0.000000   \n",
       "\n",
       "     format         4   Finance      will  considered       and         2  \n",
       "0  0.003094  0.003094  0.003094  0.006189    0.003094  0.040227  0.009283  \n",
       "1  0.000000  0.000000  0.000000  0.000000    0.000000  0.000000  0.000000  \n",
       "\n",
       "[2 rows x 117 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([tfidfBowA, tfidfBowB])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
